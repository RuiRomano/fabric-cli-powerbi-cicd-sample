{
    "nbformat": 4,
    "nbformat_minor": 5,
    "metadata": {
        "language_info": {
            "name": "python"
        },
        "kernel_info": {
            "name": "synapse_pyspark",
            "jupyter_kernel_name": null
        },
        "kernelspec": {
            "display_name": "synapse_pyspark",
            "language": null,
            "name": "synapse_pyspark"
        },
        "a365ComputeOptions": null,
        "sessionKeepAliveTimeout": 0,
        "dependencies": {
            "lakehouse": {
                "default_lakehouse": "3d020a17-ccd9-402e-9f1a-01c458a797b2",
                "default_lakehouse_name": "LH_STORE_RAW",
                "default_lakehouse_workspace_id": "84912f9a-8d9a-43d0-a945-99e74142a0f2",
                "known_lakehouses": [
                    {
                        "id": "3d020a17-ccd9-402e-9f1a-01c458a797b2"
                    }
                ]
            }
        },
        "microsoft": {
            "language": "python",
            "language_group": "synapse_pyspark",
            "ms_spell_check": {
                "ms_spell_check_language": "en"
            }
        },
        "nteract": {
            "version": "nteract-front-end@1.0.0"
        },
        "spark_compute": {
            "compute_id": "/trident/default",
            "session_options": {
                "conf": {
                    "spark.synapse.nbs.session.timeout": "1200000"
                }
            }
        }
    },
    "cells": [
        {
            "id": "0244edf7-d6f7-4def-8eca-43974a315c92",
            "cell_type": "code",
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "cellStatus": "{\"Rui Romano\":{\"session_start_time\":null,\"execution_start_time\":\"2025-03-17T13:05:23.5915219Z\",\"execution_finish_time\":\"2025-03-17T13:05:49.8870027Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}"
            },
            "source": [
                "import os\n",
                "from pyspark.sql import functions as F\n",
                "\n",
                "# Incremental tables\n",
                "csv_files = [\"customer.csv\", \"sales.csv\", \"product.csv\", \"store.csv\"]\n",
                "\n",
                "for csv_file in csv_files:\n",
                "\n",
                "    table_name = os.path.splitext(csv_file)[0]\n",
                "\n",
                "    # Read the source table\n",
                "    df = spark.read.format(\"csv\").option(\"header\",\"true\").load(f\"Files/raw/{csv_file}\")\n",
                "\n",
                "    # remove column name invalid chars\n",
                "\n",
                "    df = df.select([F.col(col).alias(col.replace(' ', '')) for col in df.columns])\n",
                "   \n",
                "    # Write to the target schema, replacing the existing table\n",
                "\n",
                "    df.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").format(\"delta\").saveAsTable(f\"raw.{table_name}\")    \n",
                "    "
            ],
            "outputs": [],
            "execution_count": 14
        }
    ]
}